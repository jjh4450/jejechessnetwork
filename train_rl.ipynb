{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ® ì²´ìŠ¤ AI ê°•í™”í•™ìŠµ (AlphaZero ìŠ¤íƒ€ì¼ MCTS)\n",
    "\n",
    "**Pre-trained CNN ëª¨ë¸ì„ MCTS ê¸°ë°˜ AlphaZero ìŠ¤íƒ€ì¼ë¡œ ê°•í™”í•™ìŠµí•©ë‹ˆë‹¤.**\n",
    "\n",
    "## í•µì‹¬ ê°œì„ ì‚¬í•­\n",
    "- **MCTS íƒìƒ‰**: ì‹œë®¬ë ˆì´ì…˜ 400íšŒ ì´ìƒìœ¼ë¡œ Policyë³´ë‹¤ ë” ë‚˜ì€ ìˆ˜ íƒìƒ‰\n",
    "- **í•™ìŠµ íƒ€ê²Ÿ**: MCTS ë°©ë¬¸ íšŸìˆ˜ ë¶„í¬ë¥¼ ì •ì±… íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©\n",
    "- **Opponent Pool**: SL ëª¨ë¸ì„ í¬í•¨í•œ ìƒëŒ€ í’€ê³¼ ëŒ€ê²°\n",
    "- **ê²Œì´íŒ…**: 55% ì´ìƒ ìŠ¹ë¥  ì‹œì—ë§Œ ëª¨ë¸ ê°±ì‹ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:45:31.158114500Z",
     "start_time": "2026-02-05T11:45:28.039368400Z"
    }
   },
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ\n",
    "from chess_model import ChessCNN, load_model\n",
    "from train_utils import (\n",
    "    train_step_mcts, evaluate_vs_opponent, check_weight_diff,\n",
    "    compute_kl_divergence, compute_masked_entropy,\n",
    "    create_tensorboard_writer\n",
    ")\n",
    "from mcts import (\n",
    "    MCTS, OpponentPool,\n",
    "    play_games_batch_with_mcts, play_games_vs_opponent_pool\n",
    ")\n",
    "import fast_chess as fc\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# CUDA ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: cuda\n",
      "ğŸ“Š GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "ğŸ’¾ VRAM: 17.1 GB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:45:31.206914700Z",
     "start_time": "2026-02-05T11:45:31.158631100Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "# =============================================================================\n",
    "LEARNING_RATE = 5e-5          # Fine-tuningìš© ë‚®ì€ í•™ìŠµë¥  (ë” ë³´ìˆ˜ì )\n",
    "LEARNING_RATE_MIN = 1e-6      # ìµœì†Œ í•™ìŠµë¥ \n",
    "NUM_ITERATIONS = 10000        # ì´ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜\n",
    "GAMES_PER_ITERATION = 8       # ë°˜ë³µë‹¹ Self-play ê²Œì„ ìˆ˜ (MCTSëŠ” ëŠë¦¬ë¯€ë¡œ ì¤„ì„)\n",
    "MAX_MOVES = 200               # ê²Œì„ë‹¹ ìµœëŒ€ ìˆ˜\n",
    "\n",
    "# =============================================================================\n",
    "# MCTS ì„¤ì • (í•µì‹¬!)\n",
    "# =============================================================================\n",
    "MCTS_SIMULATIONS = 400        # MCTS ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜ (ìµœì†Œ 400 ê¶Œì¥)\n",
    "MCTS_C_PUCT = 1.5             # UCB íƒìƒ‰ ìƒìˆ˜\n",
    "MCTS_DIRICHLET_ALPHA = 0.3    # ë£¨íŠ¸ ë…¸ì´ì¦ˆ íŒŒë¼ë¯¸í„°\n",
    "MCTS_TEMPERATURE = 1.0        # ì´ˆê¸° íƒìƒ‰ ì˜¨ë„\n",
    "MCTS_TEMP_THRESHOLD = 30      # ì´ ìˆ˜ ì´í›„ greedy ì„ íƒ\n",
    "\n",
    "# =============================================================================\n",
    "# Opponent Pool ì„¤ì •\n",
    "# =============================================================================\n",
    "OPPONENT_POOL_SIZE = 5        # ìµœëŒ€ ìƒëŒ€ ëª¨ë¸ ìˆ˜ (SL ëª¨ë¸ ì œì™¸)\n",
    "SL_MODEL_WEIGHT = 0.3         # SL ëª¨ë¸ ì„ íƒ í™•ë¥  ê°€ì¤‘ì¹˜\n",
    "\n",
    "# =============================================================================\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜\n",
    "# =============================================================================\n",
    "VALUE_LOSS_WEIGHT = 1.0       # Value Loss ê°€ì¤‘ì¹˜ (AlphaZero ìŠ¤íƒ€ì¼)\n",
    "ENTROPY_BONUS = 0.01          # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ (íƒìƒ‰ ì¥ë ¤)\n",
    "KL_PENALTY = 0.1              # KL Divergence íŒ¨ë„í‹° ì´ˆê¸°ê°’\n",
    "\n",
    "# =============================================================================\n",
    "# KL Divergence ì„¤ì • (Adaptive)\n",
    "# =============================================================================\n",
    "USE_ADAPTIVE_KL = True        # Adaptive KL íŒ¨ë„í‹° ì‚¬ìš©\n",
    "KL_TARGET = 0.001             # ëª©í‘œ KL ê°’\n",
    "KL_PENALTY_MIN = 1e-4         # ìµœì†Œ Î² ê°’\n",
    "KL_PENALTY_MAX = 10.0         # ìµœëŒ€ Î² ê°’\n",
    "\n",
    "# =============================================================================\n",
    "# í‰ê°€ ë° ì €ì¥ (ê²Œì´íŒ… ê°•í™”!)\n",
    "# =============================================================================\n",
    "EVAL_INTERVAL = 10            # í‰ê°€ ì£¼ê¸° (iteration)\n",
    "EVAL_GAMES = 50               # í‰ê°€ìš© ëŒ€ê²° ê²Œì„ ìˆ˜ (50íŒìœ¼ë¡œ ì¦ê°€ - í†µê³„ì  ìœ ì˜ì„±)\n",
    "WIN_RATE_THRESHOLD = 0.55     # ì €ì¥ ê¸°ì¤€ ìŠ¹ë¥  (55%+ ë¡œ ìƒí–¥)\n",
    "SAVE_INTERVAL = 50            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°\n",
    "\n",
    "# =============================================================================\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "# =============================================================================\n",
    "PRETRAINED_PATH = 'models/best_chess_cnn.pth'\n",
    "CHECKPOINT_PATH = 'models/chess_cnn_rl_mcts.pth'\n",
    "BEST_MODEL_PATH = 'models/best_chess_cnn_rl_mcts.pth'\n",
    "\n",
    "print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"   - í•™ìŠµë¥ : {LEARNING_RATE} (min: {LEARNING_RATE_MIN})\")\n",
    "print(f\"   - ë°˜ë³µ íšŸìˆ˜: {NUM_ITERATIONS}\")\n",
    "print(f\"   - ê²Œì„/ë°˜ë³µ: {GAMES_PER_ITERATION}\")\n",
    "print(f\"   - MCTS ì‹œë®¬ë ˆì´ì…˜: {MCTS_SIMULATIONS}íšŒ\")\n",
    "print(f\"   - ìƒëŒ€ í’€ í¬ê¸°: {OPPONENT_POOL_SIZE} + SL ëª¨ë¸\")\n",
    "print(f\"   - ê²Œì´íŒ… ì„ê³„ê°’: {WIN_RATE_THRESHOLD:.0%}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\n",
      "   - í•™ìŠµë¥ : 5e-05 (min: 1e-06)\n",
      "   - ë°˜ë³µ íšŸìˆ˜: 10000\n",
      "   - ê²Œì„/ë°˜ë³µ: 8\n",
      "   - MCTS ì‹œë®¬ë ˆì´ì…˜: 400íšŒ\n",
      "   - ìƒëŒ€ í’€ í¬ê¸°: 5 + SL ëª¨ë¸\n",
      "   - ê²Œì´íŒ… ì„ê³„ê°’: 55%\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ë¡œë“œ\n",
    "\n",
    "Pre-trained ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , Reference ëª¨ë¸(ê³ ì •)ê³¼ í•™ìŠµ ëª¨ë¸ì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:45:31.911617800Z",
     "start_time": "2026-02-05T11:45:31.208980800Z"
    }
   },
   "source": [
    "# Pre-trained ëª¨ë¸ ë¡œë“œ\n",
    "print(\"ğŸ“¥ Pre-trained ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "if os.path.exists(PRETRAINED_PATH):\n",
    "    model, checkpoint = load_model(PRETRAINED_PATH, device)\n",
    "    print(f\"   âœ… {PRETRAINED_PATH} ë¡œë“œ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ {PRETRAINED_PATH} ì—†ìŒ - ìƒˆ ëª¨ë¸ ìƒì„±\")\n",
    "    model = ChessCNN(num_channels=256).to(device)\n",
    "\n",
    "# í•™ìŠµ ëª¨ë¸ë¡œ ì„¤ì •\n",
    "model.train()\n",
    "\n",
    "# Reference ëª¨ë¸ (Pre-trained ìƒíƒœ ê³ ì •) - KL íŒ¨ë„í‹° ê³„ì‚°ìš©\n",
    "print(\"ğŸ”’ Reference ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "ref_model = ChessCNN(num_channels=256).to(device)\n",
    "ref_model.load_state_dict(model.state_dict())\n",
    "ref_model.eval()\n",
    "\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"   âœ… Reference ëª¨ë¸ ë™ê²° ì™„ë£Œ\")\n",
    "\n",
    "# SL ëª¨ë¸ (Opponent Poolìš©, ê³ ì •)\n",
    "print(\"ğŸ“¦ SL ëª¨ë¸ ìƒì„± ì¤‘ (Opponent Pool)...\")\n",
    "sl_model = ChessCNN(num_channels=256).to(device)\n",
    "sl_model.load_state_dict(model.state_dict())\n",
    "sl_model.eval()\n",
    "\n",
    "for param in sl_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"   âœ… SL ëª¨ë¸ ë™ê²° ì™„ë£Œ\")\n",
    "\n",
    "# Opponent Pool ìƒì„±\n",
    "print(\"ğŸ¯ Opponent Pool ìƒì„± ì¤‘...\")\n",
    "opponent_pool = OpponentPool(\n",
    "    sl_model=sl_model,\n",
    "    device=device,\n",
    "    max_opponents=OPPONENT_POOL_SIZE,\n",
    "    sl_model_weight=SL_MODEL_WEIGHT,\n",
    ")\n",
    "print(f\"   âœ… Opponent Pool ìƒì„± ì™„ë£Œ (SL ëª¨ë¸ í¬í•¨)\")\n",
    "\n",
    "# Best ëª¨ë¸ (í‰ê°€ìš© ìƒëŒ€)\n",
    "best_model = ChessCNN(num_channels=256).to(device)\n",
    "best_model.load_state_dict(model.state_dict())\n",
    "best_model.eval()\n",
    "print(\"   âœ… Best ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"   - ì „ì²´: {total_params:,}\")\n",
    "print(f\"   - í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… models/best_chess_cnn.pth ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ”’ Reference ëª¨ë¸ ìƒì„± ì¤‘...\n",
      "   âœ… Reference ëª¨ë¸ ë™ê²° ì™„ë£Œ\n",
      "ğŸ“¦ SL ëª¨ë¸ ìƒì„± ì¤‘ (Opponent Pool)...\n",
      "   âœ… SL ëª¨ë¸ ë™ê²° ì™„ë£Œ\n",
      "ğŸ¯ Opponent Pool ìƒì„± ì¤‘...\n",
      "   âœ… Opponent Pool ìƒì„± ì™„ë£Œ (SL ëª¨ë¸ í¬í•¨)\n",
      "   âœ… Best ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "\n",
      "ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°:\n",
      "   - ì „ì²´: 9,314,433\n",
      "   - í•™ìŠµ ê°€ëŠ¥: 9,314,433\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì˜µí‹°ë§ˆì´ì € ë° TensorBoard ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:45:33.625469500Z",
     "start_time": "2026-02-05T11:45:32.726080300Z"
    }
   },
   "source": [
    "# ì˜µí‹°ë§ˆì´ì €\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Cosine Annealing)\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=NUM_ITERATIONS,\n",
    "    eta_min=LEARNING_RATE_MIN\n",
    ")\n",
    "print(f\"ğŸ“‰ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: CosineAnnealing (max={LEARNING_RATE}, min={LEARNING_RATE_MIN})\")\n",
    "\n",
    "# AMP (Automatic Mixed Precision)\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "print(f\"âš¡ AMP: {'í™œì„±í™”' if use_amp else 'ë¹„í™œì„±í™”'}\")\n",
    "\n",
    "# TensorBoard Writer ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)\n",
    "writer = create_tensorboard_writer(\"rl_mcts\")\n",
    "print(f\"ğŸ“Š TensorBoard: {writer.log_dir}\")\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ\n",
    "start_iteration = 0\n",
    "best_win_rate = 0.0\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    if 'model_state_dict' in ckpt:\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        if 'optimizer_state_dict' in ckpt:\n",
    "            optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        if 'scheduler_state_dict' in ckpt:\n",
    "            scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "        start_iteration = ckpt.get('iteration', 0)\n",
    "        best_win_rate = ckpt.get('best_win_rate', 0.0)\n",
    "        KL_PENALTY = ckpt.get('kl_penalty', KL_PENALTY)\n",
    "        print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: iteration {start_iteration}, win_rate {best_win_rate:.2%}\")\n",
    "\n",
    "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ (iteration {start_iteration}ë¶€í„°)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‰ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: CosineAnnealing (max=5e-05, min=1e-06)\n",
      "âš¡ AMP: í™œì„±í™”\n",
      "ğŸ“Š TensorBoard: models/tensorboard/rl_mcts_20260205_114533\n",
      "\n",
      "ğŸš€ í•™ìŠµ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ (iteration 0ë¶€í„°)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•™ìŠµ ë£¨í”„\n",
    "\n",
    "Self-playë¡œ ê²Œì„ì„ ì§„í–‰í•˜ê³  A2C ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-05T11:45:34.034948100Z"
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"ğŸ® MCTS ê¸°ë°˜ ê°•í™”í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   MCTS ì‹œë®¬ë ˆì´ì…˜: {MCTS_SIMULATIONS}íšŒ\")\n",
    "print(f\"   ê²Œì´íŒ… ì„ê³„ê°’: {WIN_RATE_THRESHOLD:.0%}\")\n",
    "print(f\"   Opponent Pool: {len(opponent_pool)} ëª¨ë¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í˜„ì¬ KL Penalty (Adaptiveìš©)\n",
    "current_kl_penalty = KL_PENALTY\n",
    "\n",
    "for iteration in tqdm(range(start_iteration, NUM_ITERATIONS), desc=\"í•™ìŠµ ì§„í–‰\"):\n",
    "    # =========================================================================\n",
    "    # MCTS Self-play ê²Œì„ ì§„í–‰ (Opponent Pool ì‚¬ìš©)\n",
    "    # =========================================================================\n",
    "    trajectories, results, game_stats = play_games_vs_opponent_pool(\n",
    "        current_model=model,\n",
    "        opponent_pool=opponent_pool,\n",
    "        model_class=ChessCNN,\n",
    "        device=device,\n",
    "        num_games=GAMES_PER_ITERATION,\n",
    "        num_simulations=MCTS_SIMULATIONS,\n",
    "        c_puct=MCTS_C_PUCT,\n",
    "        temperature=MCTS_TEMPERATURE,\n",
    "        temperature_threshold=MCTS_TEMP_THRESHOLD,\n",
    "        max_moves=MAX_MOVES,\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # AlphaZero ìŠ¤íƒ€ì¼ í•™ìŠµ ìŠ¤í… (MCTS ì •ì±… íƒ€ê²Ÿ)\n",
    "    # =========================================================================\n",
    "    train_metrics = train_step_mcts(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        optimizer=optimizer,\n",
    "        trajectories=trajectories,\n",
    "        results=results,\n",
    "        device=device,\n",
    "        value_loss_weight=VALUE_LOSS_WEIGHT,\n",
    "        entropy_bonus=ENTROPY_BONUS,\n",
    "        kl_penalty=current_kl_penalty,\n",
    "        max_grad_norm=1.0,\n",
    "        scaler=scaler,\n",
    "        use_amp=use_amp\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "    # =========================================================================\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Adaptive KL Penalty\n",
    "    # =========================================================================\n",
    "    if USE_ADAPTIVE_KL:\n",
    "        avg_kl = train_metrics['kl_div']\n",
    "        if avg_kl > KL_TARGET * 1.5:\n",
    "            current_kl_penalty = min(current_kl_penalty * 1.5, KL_PENALTY_MAX)\n",
    "        elif avg_kl < KL_TARGET * 0.5:\n",
    "            current_kl_penalty = max(current_kl_penalty / 1.5, KL_PENALTY_MIN)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TensorBoard ë¡œê¹…\n",
    "    # =========================================================================\n",
    "    writer.add_scalar('Loss/Total', train_metrics['total_loss'], iteration)\n",
    "    writer.add_scalar('Loss/Policy', train_metrics['policy_loss'], iteration)\n",
    "    writer.add_scalar('Loss/Value', train_metrics['value_loss'], iteration)\n",
    "    writer.add_scalar('Entropy', train_metrics['entropy'], iteration)\n",
    "    writer.add_scalar('KL_Divergence', train_metrics['kl_div'], iteration)\n",
    "    writer.add_scalar('KL_Penalty', current_kl_penalty, iteration)\n",
    "    writer.add_scalar('Learning_Rate', current_lr, iteration)\n",
    "    writer.add_scalar('Games/Avg_Moves', game_stats['avg_moves'], iteration)\n",
    "    writer.add_scalar('Games/Wins', game_stats['wins'], iteration)\n",
    "    writer.add_scalar('Games/Losses', game_stats['losses'], iteration)\n",
    "    writer.add_scalar('Games/Draws', game_stats['draws'], iteration)\n",
    "    writer.add_scalar('Games/Win_Rate', game_stats['win_rate'], iteration)\n",
    "    writer.add_scalar('Opponent_Pool_Size', len(opponent_pool), iteration)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # í‰ê°€ ë° ê²Œì´íŒ… (ì£¼ê¸°ì )\n",
    "    # =========================================================================\n",
    "    if (iteration + 1) % EVAL_INTERVAL == 0:\n",
    "        # SL ëª¨ë¸ê³¼ ëŒ€ê²° (ê¸°ì¤€ì )\n",
    "        win_rate_vs_sl = evaluate_vs_opponent(\n",
    "            current_model=model,\n",
    "            opponent_model=sl_model,\n",
    "            device=device,\n",
    "            num_games=EVAL_GAMES,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        # Best ëª¨ë¸ê³¼ ëŒ€ê²°\n",
    "        win_rate_vs_best = evaluate_vs_opponent(\n",
    "            current_model=model,\n",
    "            opponent_model=best_model,\n",
    "            device=device,\n",
    "            num_games=EVAL_GAMES,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        writer.add_scalar('Eval/Win_Rate_vs_SL', win_rate_vs_sl, iteration)\n",
    "        writer.add_scalar('Eval/Win_Rate_vs_Best', win_rate_vs_best, iteration)\n",
    "        \n",
    "        # ê²Œì´íŒ…: 55% ì´ìƒ ìŠ¹ë¥  ì‹œì—ë§Œ ëª¨ë¸ ê°±ì‹ \n",
    "        if win_rate_vs_best >= WIN_RATE_THRESHOLD:\n",
    "            # Opponent Poolì— ì¶”ê°€\n",
    "            opponent_pool.add_opponent(model, win_rate_vs_best)\n",
    "            \n",
    "            # Best ëª¨ë¸ ê°±ì‹ \n",
    "            if win_rate_vs_best > best_win_rate:\n",
    "                best_win_rate = win_rate_vs_best\n",
    "                best_model.load_state_dict(model.state_dict())\n",
    "                \n",
    "                # Best ëª¨ë¸ ì €ì¥\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'iteration': iteration,\n",
    "                    'win_rate': win_rate_vs_best,\n",
    "                    'kl_penalty': current_kl_penalty,\n",
    "                }, BEST_MODEL_PATH)\n",
    "                \n",
    "                tqdm.write(f\"ğŸ† [Iter {iteration+1}] ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸! \"\n",
    "                          f\"vs Best: {win_rate_vs_best:.2%}, vs SL: {win_rate_vs_sl:.2%}\")\n",
    "            else:\n",
    "                tqdm.write(f\"ğŸ“ˆ [Iter {iteration+1}] Opponent Poolì— ì¶”ê°€! \"\n",
    "                          f\"vs Best: {win_rate_vs_best:.2%}\")\n",
    "        else:\n",
    "            tqdm.write(f\"ğŸ“Š [Iter {iteration+1}] ê²Œì´íŒ… ì‹¤íŒ¨ - \"\n",
    "                      f\"vs Best: {win_rate_vs_best:.2%} (í•„ìš”: {WIN_RATE_THRESHOLD:.0%}), \"\n",
    "                      f\"vs SL: {win_rate_vs_sl:.2%}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì£¼ê¸°ì )\n",
    "    # =========================================================================\n",
    "    if (iteration + 1) % SAVE_INTERVAL == 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'iteration': iteration + 1,\n",
    "            'best_win_rate': best_win_rate,\n",
    "            'kl_penalty': current_kl_penalty,\n",
    "        }, CHECKPOINT_PATH)\n",
    "        tqdm.write(f\"ğŸ’¾ [Iter {iteration+1}] ì²´í¬í¬ì¸íŠ¸ ì €ì¥\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "    # =========================================================================\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        weight_diff = check_weight_diff(model, ref_model)\n",
    "        tqdm.write(f\"ğŸ“ˆ [Iter {iteration+1}] Loss: {train_metrics['total_loss']:.4f}, \"\n",
    "                   f\"LR: {current_lr:.2e}, \"\n",
    "                   f\"KL: {train_metrics['kl_div']:.6f}, \"\n",
    "                   f\"Moves: {game_stats['avg_moves']:.1f}, \"\n",
    "                   f\"Pool: {len(opponent_pool)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# ìµœì¢… ì €ì¥\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'iteration': NUM_ITERATIONS,\n",
    "    'best_win_rate': best_win_rate,\n",
    "    'kl_penalty': current_kl_penalty,\n",
    "}, CHECKPOINT_PATH)\n",
    "print(f\"ğŸ’¾ ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {CHECKPOINT_PATH}\")\n",
    "\n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ì§„í–‰:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ìƒ˜í”Œ ê²Œì„ ì‹œê°í™”\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒ˜í”Œ ê²Œì„ì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def play_demo_game_mcts(model, device, num_simulations=100, max_moves=100, verbose=True):\n",
    "    \"\"\"MCTSë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œ ê²Œì„ì„ ì§„í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    state = fc.create_initial_state()\n",
    "    model.eval()\n",
    "    \n",
    "    mcts = MCTS(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        num_simulations=num_simulations,\n",
    "        c_puct=1.5,\n",
    "        temperature=0.0,  # Greedy for demo\n",
    "    )\n",
    "    \n",
    "    moves_history = []\n",
    "    \n",
    "    for move_num in range(max_moves):\n",
    "        if fc.is_game_over(state):\n",
    "            break\n",
    "        \n",
    "        mask_tensor = torch.from_numpy(fc.legal_move_mask_fast(state))\n",
    "        \n",
    "        if mask_tensor.sum() == 0:\n",
    "            break\n",
    "        \n",
    "        # MCTS íƒìƒ‰\n",
    "        action, mcts_probs, value = mcts.select_action(state, add_noise=False)\n",
    "        \n",
    "        # ìˆ˜ ë³€í™˜\n",
    "        from_sq = action // 64\n",
    "        to_sq = action % 64\n",
    "        files = \"abcdefgh\"\n",
    "        ranks = \"12345678\"\n",
    "        move_str = f\"{files[from_sq & 7]}{ranks[from_sq >> 3]}{files[to_sq & 7]}{ranks[to_sq >> 3]}\"\n",
    "        moves_history.append(move_str)\n",
    "        \n",
    "        fc.make_move(state, action)\n",
    "    \n",
    "    # ê²°ê³¼\n",
    "    result = fc.get_result(state)\n",
    "    if result == 1.0:\n",
    "        result_str = \"1-0 (ë°± ìŠ¹)\"\n",
    "    elif result == 0.0:\n",
    "        result_str = \"0-1 (í‘ ìŠ¹)\"\n",
    "    else:\n",
    "        result_str = \"1/2-1/2 (ë¬´ìŠ¹ë¶€)\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ğŸ® ìƒ˜í”Œ ê²Œì„ ê²°ê³¼: {result_str}\")\n",
    "        print(f\"   ì´ {len(moves_history)}ìˆ˜\")\n",
    "        print(f\"   ìˆ˜ìˆœ: {' '.join(moves_history[:20])}{'...' if len(moves_history) > 20 else ''}\")\n",
    "    \n",
    "    return moves_history, result_str\n",
    "\n",
    "# ìƒ˜í”Œ ê²Œì„ 3ê°œ ì§„í–‰ (MCTS ì‚¬ìš©)\n",
    "print(\"ğŸ² MCTS ìƒ˜í”Œ ê²Œì„ ì§„í–‰ ì¤‘...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nê²Œì„ #{i+1}\")\n",
    "    play_demo_game_mcts(model, device, num_simulations=100)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
