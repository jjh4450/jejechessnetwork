{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C 강화학습\n",
    "\n",
    "Pre-trained ChessCNN 모델을 A2C (Advantage Actor-Critic) 알고리즘과 Self-play로 강화학습합니다.\n",
    "\n",
    "## 알고리즘 개요\n",
    "- **Advantage**: A(s,a) = R - V(s)\n",
    "- **Policy Loss**: -log π(a|s) × A(s,a)\n",
    "- **Value Loss**: (V(s) - R)²\n",
    "- **Self-play**: 현재 네트워크 vs 현재 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T20:07:06.970595700Z",
     "start_time": "2026-02-03T20:07:05.626897200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA 사용 가능!\n",
      "   GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "   GPU 메모리: 15.93 GB\n",
      "\n",
      "사용 장치: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import chess\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from preprocessing import (\n",
    "    board_to_tensor,\n",
    "    legal_move_mask,\n",
    "    action_index_to_move,\n",
    "    move_to_action_index,\n",
    "    ACTION_SPACE_SIZE\n",
    ")\n",
    "from chess_model import ChessCNN, load_model\n",
    "from train_utils import compute_masked_entropy, get_masked_log_probs\n",
    "\n",
    "# 시드 고정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 디바이스 설정\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✅ CUDA 사용 가능!\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"⚠️  CUDA 사용 불가 - CPU 사용\")\n",
    "\n",
    "print(f\"\\n사용 장치: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의 및 Pre-trained 가중치 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T20:07:07.526324400Z",
     "start_time": "2026-02-03T20:07:06.972142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pre-trained 모델 로드 완료!\n",
      "   원본 Epoch: 5\n",
      "   원본 Val Loss: 2.6870\n",
      "   원본 Val Accuracy: 44.69%\n",
      "\n",
      "모델 파라미터 수: 9,314,433\n"
     ]
    }
   ],
   "source": [
    "# ChessCNN은 chess_model.py에서 import됨\n",
    "# Pre-trained 가중치 로드\n",
    "PRETRAINED_PATH = Path(\"models/best_chess_cnn.pth\")\n",
    "\n",
    "if PRETRAINED_PATH.exists():\n",
    "    model, checkpoint = load_model(PRETRAINED_PATH, device=device)\n",
    "    print(f\"✅ Pre-trained 모델 로드 완료!\")\n",
    "    print(f\"   원본 Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"   원본 Val Loss: {checkpoint.get('val_loss', 'N/A'):.4f}\")\n",
    "    print(f\"   원본 Val Accuracy: {checkpoint.get('val_accuracy', 'N/A')*100:.2f}%\")\n",
    "else:\n",
    "    # Pre-trained 모델이 없으면 랜덤 초기화\n",
    "    model = ChessCNN(num_channels=256).to(device)\n",
    "    print(f\"⚠️  Pre-trained 모델을 찾을 수 없습니다: {PRETRAINED_PATH}\")\n",
    "    print(f\"   랜덤 초기화로 시작합니다.\")\n",
    "\n",
    "# 강화학습을 위해 학습 모드로 전환\n",
    "model.train()\n",
    "\n",
    "print(f\"\\n모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-play 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T20:07:08.621929900Z",
     "start_time": "2026-02-03T20:07:08.217343400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "게임 결과: 0.0 (1=백승, -1=흑승, 0=무승부)\n",
      "총 수: 80\n",
      "최종 보드:\n",
      ". . . . . . . k\n",
      "p b . . . p . p\n",
      ". . . p . . . .\n",
      ". . p . . . . .\n",
      ". . . . . n . .\n",
      ". . . . . . . .\n",
      ". . . . r . r .\n",
      ". . . . . K . .\n"
     ]
    }
   ],
   "source": [
    "def select_action(policy_logits, mask, temperature=1.0):\n",
    "    \"\"\"\n",
    "    정책에서 행동을 샘플링합니다.\n",
    "    \n",
    "    Args:\n",
    "        policy_logits: (4096,) 정책 로짓\n",
    "        mask: (4096,) 합법 수 마스크\n",
    "        temperature: 탐색 온도 (높을수록 랜덤, 낮을수록 greedy)\n",
    "    \n",
    "    Returns:\n",
    "        action: 선택된 액션 인덱스\n",
    "        log_prob: 선택된 액션의 log 확률\n",
    "        probs: 전체 확률 분포\n",
    "    \"\"\"\n",
    "    # 불법 수 마스킹\n",
    "    masked_logits = policy_logits.clone()\n",
    "    masked_logits[~mask.bool()] = float('-inf')\n",
    "    \n",
    "    # Temperature 적용\n",
    "    if temperature != 1.0:\n",
    "        masked_logits = masked_logits / temperature\n",
    "    \n",
    "    # Softmax로 확률 계산\n",
    "    probs = F.softmax(masked_logits, dim=-1)\n",
    "    \n",
    "    # 확률적 샘플링\n",
    "    action = torch.multinomial(probs, 1).item()\n",
    "    log_prob = torch.log(probs[action] + 1e-10)\n",
    "    \n",
    "    return action, log_prob, probs\n",
    "\n",
    "\n",
    "def get_game_result(board: chess.Board) -> float:\n",
    "    \"\"\"\n",
    "    게임 결과를 반환합니다.\n",
    "    \n",
    "    Returns:\n",
    "        +1.0: 백 승\n",
    "        -1.0: 흑 승\n",
    "         0.0: 무승부\n",
    "    \"\"\"\n",
    "    result = board.result()\n",
    "    if result == \"1-0\":\n",
    "        return 1.0\n",
    "    elif result == \"0-1\":\n",
    "        return -1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def find_promotion_move(board: chess.Board, action_idx: int) -> chess.Move:\n",
    "    \"\"\"\n",
    "    프로모션이 필요한 경우 Queen 프로모션으로 변환합니다.\n",
    "    \"\"\"\n",
    "    base_move = action_index_to_move(action_idx)\n",
    "    \n",
    "    # 합법 수 중에서 같은 from/to를 가진 수 찾기\n",
    "    for move in board.legal_moves:\n",
    "        if move.from_square == base_move.from_square and move.to_square == base_move.to_square:\n",
    "            # 프로모션인 경우 Queen 승격만 허용\n",
    "            if move.promotion is not None:\n",
    "                return chess.Move(move.from_square, move.to_square, promotion=chess.QUEEN)\n",
    "            return move\n",
    "    \n",
    "    # 찾지 못하면 기본 수 반환 (에러 상황)\n",
    "    return base_move\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def play_game(model, device, temperature=1.0, max_moves=200):\n",
    "    \"\"\"\n",
    "    Self-play로 한 게임을 진행합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: ChessCNN 모델\n",
    "        device: 연산 장치\n",
    "        temperature: 탐색 온도\n",
    "        max_moves: 최대 수 제한 (무한 루프 방지)\n",
    "    \n",
    "    Returns:\n",
    "        trajectory: [(state_tensor, action, log_prob, value), ...]\n",
    "        result: 게임 결과 (+1, 0, -1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    board = chess.Board()\n",
    "    trajectory = []\n",
    "    \n",
    "    move_count = 0\n",
    "    while not board.is_game_over() and move_count < max_moves:\n",
    "        # 상태를 텐서로 변환\n",
    "        state = board_to_tensor(board)\n",
    "        mask = legal_move_mask(board)\n",
    "        \n",
    "        state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0).to(device)\n",
    "        \n",
    "        # 모델 추론\n",
    "        policy_logits, value = model(state_tensor, mask_tensor)\n",
    "        policy_logits = policy_logits.squeeze(0)\n",
    "        value = value.squeeze()\n",
    "        \n",
    "        # 행동 선택\n",
    "        action, log_prob, _ = select_action(policy_logits, mask_tensor.squeeze(0), temperature)\n",
    "        \n",
    "        # trajectory에 저장 (나중에 학습에 사용)\n",
    "        trajectory.append({\n",
    "            'state': state_tensor.squeeze(0),  # (18, 8, 8)\n",
    "            'mask': mask_tensor.squeeze(0),    # (4096,)\n",
    "            'action': action,\n",
    "            'log_prob': log_prob,\n",
    "            'value': value,\n",
    "            'turn': board.turn  # WHITE=True, BLACK=False\n",
    "        })\n",
    "        \n",
    "        # 수 실행\n",
    "        move = find_promotion_move(board, action)\n",
    "        board.push(move)\n",
    "        move_count += 1\n",
    "    \n",
    "    # 게임 결과 (무승부 처리)\n",
    "    if move_count >= max_moves:\n",
    "        result = 0.0  # 최대 수 초과 시 무승부 처리\n",
    "    else:\n",
    "        result = get_game_result(board)\n",
    "    \n",
    "    return trajectory, result, board\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 배치 GPU 추론을 위한 Vectorized Environment\n",
    "# =============================================================================\n",
    "\n",
    "class VectorizedChessEnv:\n",
    "    \"\"\"\n",
    "    여러 체스 게임을 동시에 관리하는 벡터화된 환경\n",
    "    \n",
    "    모든 게임의 상태를 배치로 모아서 GPU에서 한 번에 추론합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_envs, max_moves=200):\n",
    "        self.num_envs = num_envs\n",
    "        self.max_moves = max_moves\n",
    "        self.boards = [chess.Board() for _ in range(num_envs)]\n",
    "        self.trajectories = [[] for _ in range(num_envs)]\n",
    "        self.dones = [False] * num_envs\n",
    "        self.move_counts = [0] * num_envs\n",
    "    \n",
    "    def get_states_batch(self, device):\n",
    "        \"\"\"\n",
    "        모든 활성 게임의 보드 상태를 배치 텐서로 반환합니다.\n",
    "        \n",
    "        Returns:\n",
    "            states: (num_active, 18, 8, 8) 텐서\n",
    "            masks: (num_active, 4096) 텐서\n",
    "            active_indices: 활성 게임 인덱스 리스트\n",
    "        \"\"\"\n",
    "        active_indices = [i for i in range(self.num_envs) if not self.dones[i]]\n",
    "        \n",
    "        if not active_indices:\n",
    "            return None, None, []\n",
    "        \n",
    "        states = []\n",
    "        masks = []\n",
    "        \n",
    "        for idx in active_indices:\n",
    "            state = board_to_tensor(self.boards[idx])\n",
    "            mask = legal_move_mask(self.boards[idx])\n",
    "            states.append(state)\n",
    "            masks.append(mask)\n",
    "        \n",
    "        # 배치 텐서로 변환\n",
    "        states_tensor = torch.from_numpy(np.stack(states)).to(device)  # (num_active, 18, 8, 8)\n",
    "        masks_tensor = torch.from_numpy(np.stack(masks)).to(device)    # (num_active, 4096)\n",
    "        \n",
    "        return states_tensor, masks_tensor, active_indices\n",
    "    \n",
    "    def step_batch(self, actions, policy_logits_batch, values_batch, active_indices, temperature):\n",
    "        \"\"\"\n",
    "        배치로 받은 액션을 각 게임에 적용합니다.\n",
    "        \n",
    "        Args:\n",
    "            actions: (num_active,) 액션 인덱스 리스트\n",
    "            policy_logits_batch: (num_active, 4096) 정책 로짓 배치\n",
    "            values_batch: (num_active, 1) 가치 배치\n",
    "            active_indices: 활성 게임 인덱스 리스트\n",
    "            temperature: 탐색 온도\n",
    "        \"\"\"\n",
    "        for batch_idx, env_idx in enumerate(active_indices):\n",
    "            if self.dones[env_idx]:\n",
    "                continue\n",
    "            \n",
    "            board = self.boards[env_idx]\n",
    "            \n",
    "            # 상태와 마스크 (trajectory 저장용)\n",
    "            state = board_to_tensor(board)\n",
    "            mask = legal_move_mask(board)\n",
    "            state_tensor = torch.from_numpy(state).to(policy_logits_batch.device)\n",
    "            mask_tensor = torch.from_numpy(mask).to(policy_logits_batch.device)\n",
    "            \n",
    "            # 액션 선택 (배치에서 해당 인덱스 추출)\n",
    "            policy_logits = policy_logits_batch[batch_idx]  # (4096,)\n",
    "            value = values_batch[batch_idx].item() if values_batch.dim() > 0 else values_batch[batch_idx]\n",
    "            action = actions[batch_idx]\n",
    "            \n",
    "            # Log 확률 계산 (trajectory 저장용)\n",
    "            masked_logits = policy_logits.clone()\n",
    "            masked_logits[~mask_tensor.bool()] = float('-inf')\n",
    "            if temperature != 1.0:\n",
    "                masked_logits = masked_logits / temperature\n",
    "            probs = F.softmax(masked_logits, dim=-1)\n",
    "            log_prob = torch.log(probs[action] + 1e-10)\n",
    "            \n",
    "            # Trajectory 저장 (GPU에 그대로 저장)\n",
    "            self.trajectories[env_idx].append({\n",
    "                'state': state_tensor,\n",
    "                'mask': mask_tensor,\n",
    "                'action': action,\n",
    "                'log_prob': log_prob,\n",
    "                'value': value,\n",
    "                'turn': board.turn\n",
    "            })\n",
    "            \n",
    "            # 수 실행\n",
    "            move = find_promotion_move(board, action)\n",
    "            board.push(move)\n",
    "            self.move_counts[env_idx] += 1\n",
    "            \n",
    "            # 게임 종료 체크\n",
    "            if board.is_game_over() or self.move_counts[env_idx] >= self.max_moves:\n",
    "                self.dones[env_idx] = True\n",
    "    \n",
    "    def get_trajectories(self):\n",
    "        \"\"\"모든 게임의 trajectory 반환\"\"\"\n",
    "        return self.trajectories\n",
    "    \n",
    "    def get_results(self):\n",
    "        \"\"\"모든 게임의 결과 반환\"\"\"\n",
    "        results = []\n",
    "        for i in range(self.num_envs):\n",
    "            if self.move_counts[i] >= self.max_moves:\n",
    "                results.append(0.0)  # 최대 수 초과 시 무승부\n",
    "            else:\n",
    "                results.append(get_game_result(self.boards[i]))\n",
    "        return results\n",
    "    \n",
    "    def all_done(self):\n",
    "        \"\"\"모든 게임이 종료되었는지 확인\"\"\"\n",
    "        return all(self.dones)\n",
    "\n",
    "\n",
    "def select_actions_batch(policy_logits_batch, masks_batch, temperature=1.0):\n",
    "    \"\"\"\n",
    "    배치로 정책에서 행동을 샘플링합니다.\n",
    "    \n",
    "    Args:\n",
    "        policy_logits_batch: (batch_size, 4096) 정책 로짓 배치\n",
    "        masks_batch: (batch_size, 4096) 합법 수 마스크 배치\n",
    "        temperature: 탐색 온도\n",
    "    \n",
    "    Returns:\n",
    "        actions: (batch_size,) 선택된 액션 인덱스 리스트\n",
    "    \"\"\"\n",
    "    batch_size = policy_logits_batch.size(0)\n",
    "    actions = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        policy_logits = policy_logits_batch[i]  # (4096,)\n",
    "        mask = masks_batch[i]  # (4096,)\n",
    "        \n",
    "        # 불법 수 마스킹\n",
    "        masked_logits = policy_logits.clone()\n",
    "        masked_logits[~mask.bool()] = float('-inf')\n",
    "        \n",
    "        # Temperature 적용\n",
    "        if temperature != 1.0:\n",
    "            masked_logits = masked_logits / temperature\n",
    "        \n",
    "        # Softmax로 확률 계산\n",
    "        probs = F.softmax(masked_logits, dim=-1)\n",
    "        \n",
    "        # 확률적 샘플링\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        actions.append(action)\n",
    "    \n",
    "    return actions\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def play_games_vectorized(model, device, num_games, temperature=1.0, max_moves=200):\n",
    "    \"\"\"\n",
    "    배치 GPU 추론으로 여러 게임을 동시에 진행합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: ChessCNN 모델\n",
    "        device: 연산 장치\n",
    "        num_games: 동시에 진행할 게임 수\n",
    "        temperature: 탐색 온도\n",
    "        max_moves: 최대 수 제한\n",
    "    \n",
    "    Returns:\n",
    "        all_trajectories: 모든 게임의 trajectory 리스트\n",
    "        results: 각 게임의 결과\n",
    "        stats: 통계 정보\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    env = VectorizedChessEnv(num_games, max_moves)\n",
    "    \n",
    "    # 모든 게임이 종료될 때까지 반복\n",
    "    while not env.all_done():\n",
    "        # 1. 모든 활성 게임의 상태를 배치로 수집\n",
    "        states_batch, masks_batch, active_indices = env.get_states_batch(device)\n",
    "        \n",
    "        if states_batch is None:\n",
    "            break\n",
    "        \n",
    "        # 2. GPU에서 배치 추론\n",
    "        policy_logits_batch, values_batch = model(states_batch, masks_batch)\n",
    "        # policy_logits_batch: (num_active, 4096)\n",
    "        # values_batch: (num_active, 1)\n",
    "        \n",
    "        # 3. 각 게임에 대해 액션 선택\n",
    "        actions = select_actions_batch(policy_logits_batch, masks_batch, temperature)\n",
    "        \n",
    "        # 4. 액션 적용\n",
    "        env.step_batch(actions, policy_logits_batch, values_batch, active_indices, temperature)\n",
    "    \n",
    "    # 결과 수집\n",
    "    all_trajectories = env.get_trajectories()\n",
    "    results = env.get_results()\n",
    "    \n",
    "    # 통계 계산\n",
    "    total_moves = sum(len(traj) for traj in all_trajectories)\n",
    "    white_wins = sum(1 for r in results if r > 0)\n",
    "    black_wins = sum(1 for r in results if r < 0)\n",
    "    draws = sum(1 for r in results if r == 0)\n",
    "    \n",
    "    stats = {\n",
    "        'num_games': num_games,\n",
    "        'avg_moves': total_moves / num_games if num_games > 0 else 0,\n",
    "        'white_wins': white_wins,\n",
    "        'black_wins': black_wins,\n",
    "        'draws': draws\n",
    "    }\n",
    "    \n",
    "    return all_trajectories, results, stats\n",
    "\n",
    "\n",
    "def play_games_sequential(model, device, num_games, temperature=1.0, max_moves=200):\n",
    "    \"\"\"\n",
    "    순차적으로 여러 게임을 진행합니다 (기존 로직, 롤백용).\n",
    "    \n",
    "    Returns:\n",
    "        all_trajectories: 모든 게임의 trajectory 리스트\n",
    "        results: 각 게임의 결과\n",
    "        stats: 통계 정보\n",
    "    \"\"\"\n",
    "    all_trajectories = []\n",
    "    results = []\n",
    "    total_moves = 0\n",
    "    white_wins = 0\n",
    "    black_wins = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "        trajectory, result, board = play_game(model, device, temperature, max_moves)\n",
    "        all_trajectories.append(trajectory)\n",
    "        results.append(result)\n",
    "        total_moves += len(trajectory)\n",
    "        \n",
    "        if result > 0:\n",
    "            white_wins += 1\n",
    "        elif result < 0:\n",
    "            black_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    stats = {\n",
    "        'num_games': num_games,\n",
    "        'avg_moves': total_moves / num_games,\n",
    "        'white_wins': white_wins,\n",
    "        'black_wins': black_wins,\n",
    "        'draws': draws\n",
    "    }\n",
    "    \n",
    "    return all_trajectories, results, stats\n",
    "\n",
    "\n",
    "def play_games_batch(model, device, num_games, temperature=1.0, num_envs=1, max_moves=200):\n",
    "    \"\"\"\n",
    "    여러 게임을 Self-play로 진행합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: ChessCNN 모델\n",
    "        device: 연산 장치\n",
    "        num_games: 총 게임 수\n",
    "        temperature: 탐색 온도\n",
    "        num_envs: 동시에 진행할 게임 수 (배치 크기)\n",
    "                  num_envs=1: 순차 실행 (기존 동작, 롤백용)\n",
    "                  num_envs>1: 배치 GPU 추론\n",
    "        max_moves: 최대 수 제한\n",
    "    \n",
    "    Returns:\n",
    "        all_trajectories: 모든 게임의 trajectory 리스트\n",
    "        results: 각 게임의 결과\n",
    "        stats: 통계 정보\n",
    "    \"\"\"\n",
    "    if num_envs == 1:\n",
    "        # 순차 실행 (기존 동작, 롤백용)\n",
    "        return play_games_sequential(model, device, num_games, temperature, max_moves)\n",
    "    else:\n",
    "        # 배치 GPU 추론\n",
    "        # num_games를 num_envs 단위로 나누어 처리\n",
    "        all_trajectories = []\n",
    "        all_results = []\n",
    "        total_moves = 0\n",
    "        white_wins = 0\n",
    "        black_wins = 0\n",
    "        draws = 0\n",
    "        \n",
    "        num_batches = (num_games + num_envs - 1) // num_envs  # 올림\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_size = min(num_envs, num_games - batch_idx * num_envs)\n",
    "            if batch_size <= 0:\n",
    "                break\n",
    "            \n",
    "            trajectories, results, stats = play_games_vectorized(\n",
    "                model, device, batch_size, temperature, max_moves\n",
    "            )\n",
    "            \n",
    "            all_trajectories.extend(trajectories)\n",
    "            all_results.extend(results)\n",
    "            total_moves += stats['avg_moves'] * batch_size\n",
    "            white_wins += stats['white_wins']\n",
    "            black_wins += stats['black_wins']\n",
    "            draws += stats['draws']\n",
    "        \n",
    "        final_stats = {\n",
    "            'num_games': num_games,\n",
    "            'avg_moves': total_moves / num_games if num_games > 0 else 0,\n",
    "            'white_wins': white_wins,\n",
    "            'black_wins': black_wins,\n",
    "            'draws': draws\n",
    "        }\n",
    "        \n",
    "        return all_trajectories, all_results, final_stats\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 모델 평가 함수 (승률 기반 저장용)\n",
    "# =============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def play_game_two_models(current_model, opponent_model, device, temperature=0.5, max_moves=200):\n",
    "    \"\"\"\n",
    "    두 모델이 대결하는 게임을 진행합니다.\n",
    "    \n",
    "    Args:\n",
    "        current_model: 현재 모델 (평가 대상)\n",
    "        opponent_model: 상대 모델\n",
    "        device: 연산 장치\n",
    "        temperature: 탐색 온도\n",
    "        max_moves: 최대 수 제한\n",
    "    \n",
    "    Returns:\n",
    "        result: 게임 결과 (+1=현재 모델 승, -1=상대 승, 0=무승부)\n",
    "    \"\"\"\n",
    "    current_model.eval()\n",
    "    opponent_model.eval()\n",
    "    board = chess.Board()\n",
    "    \n",
    "    move_count = 0\n",
    "    while not board.is_game_over() and move_count < max_moves:\n",
    "        # 상태를 텐서로 변환\n",
    "        state = board_to_tensor(board)\n",
    "        mask = legal_move_mask(board)\n",
    "        \n",
    "        state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0).to(device)\n",
    "        \n",
    "        # 현재 차례의 모델 선택\n",
    "        if board.turn == chess.WHITE:\n",
    "            # 백 차례: 현재 모델\n",
    "            policy_logits, _ = current_model(state_tensor, mask_tensor)\n",
    "        else:\n",
    "            # 흑 차례: 상대 모델\n",
    "            policy_logits, _ = opponent_model(state_tensor, mask_tensor)\n",
    "        \n",
    "        policy_logits = policy_logits.squeeze(0)\n",
    "        \n",
    "        # 행동 선택\n",
    "        action, _, _ = select_action(policy_logits, mask_tensor.squeeze(0), temperature)\n",
    "        \n",
    "        # 수 실행\n",
    "        move = find_promotion_move(board, action)\n",
    "        board.push(move)\n",
    "        move_count += 1\n",
    "    \n",
    "    # 게임 결과 (현재 모델 관점)\n",
    "    if move_count >= max_moves:\n",
    "        result = 0.0  # 최대 수 초과 시 무승부\n",
    "    else:\n",
    "        game_result = get_game_result(board)\n",
    "        # 현재 모델이 백이었으면 game_result 그대로, 흑이었으면 -game_result\n",
    "        # (play_game_two_models는 항상 현재 모델이 백으로 시작)\n",
    "        result = game_result\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_vs_opponent(current_model, opponent_model, device, num_games=10, temperature=0.5):\n",
    "    \"\"\"\n",
    "    현재 모델 vs 상대 모델 대결\n",
    "    \n",
    "    Args:\n",
    "        current_model: 현재 모델 (평가 대상)\n",
    "        opponent_model: 상대 모델\n",
    "        device: 연산 장치\n",
    "        num_games: 대결 게임 수\n",
    "        temperature: 탐색 온도\n",
    "    \n",
    "    Returns:\n",
    "        win_rate: 현재 모델의 승률 (0.0 ~ 1.0, 무승부 = 0.5점)\n",
    "    \"\"\"\n",
    "    current_model.eval()\n",
    "    opponent_model.eval()\n",
    "    \n",
    "    total_score = 0.0\n",
    "    \n",
    "    for game_idx in range(num_games):\n",
    "        # 현재 모델이 백/흑 번갈아 플레이\n",
    "        if game_idx % 2 == 0:\n",
    "            # 현재 모델이 백\n",
    "            result = play_game_two_models(current_model, opponent_model, device, temperature)\n",
    "        else:\n",
    "            # 현재 모델이 흑 (상대가 백)\n",
    "            result = play_game_two_models(opponent_model, current_model, device, temperature)\n",
    "            result = -result  # 결과 반전 (현재 모델 관점)\n",
    "        \n",
    "        # 점수 계산: 승=1.0, 무=0.5, 패=0.0\n",
    "        if result > 0:\n",
    "            score = 1.0\n",
    "        elif result < 0:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            score = 0.5\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    win_rate = total_score / num_games\n",
    "    return win_rate\n",
    "\n",
    "\n",
    "# 테스트: 1 게임 플레이\n",
    "print(\"테스트 게임 진행 중...\")\n",
    "trajectory, result, board = play_game(model, device, temperature=1.0)\n",
    "print(f\"게임 결과: {result} (1=백승, -1=흑승, 0=무승부)\")\n",
    "print(f\"총 수: {len(trajectory)}\")\n",
    "print(f\"최종 보드:\\n{board}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T20:07:08.673249700Z",
     "start_time": "2026-02-03T20:07:08.622970800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C 학습 함수 정의 완료!\n"
     ]
    }
   ],
   "source": [
    "def compute_a2c_loss(model, trajectories, results, device, \n",
    "                      value_loss_weight=0.5, entropy_bonus=0.01):\n",
    "    \"\"\"\n",
    "    A2C 손실을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        model: ChessCNN 모델\n",
    "        trajectories: 게임 trajectory 리스트\n",
    "        results: 각 게임의 결과 리스트\n",
    "        value_loss_weight: Value Loss 가중치\n",
    "        entropy_bonus: 엔트로피 보너스 (탐색 장려)\n",
    "    \n",
    "    Returns:\n",
    "        total_loss: 전체 손실\n",
    "        policy_loss: 정책 손실\n",
    "        value_loss: 가치 손실\n",
    "        entropy: 평균 엔트로피\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_policy_loss = 0.0\n",
    "    total_value_loss = 0.0\n",
    "    total_entropy = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for trajectory, result in zip(trajectories, results):\n",
    "        for i, step in enumerate(trajectory):\n",
    "            state = step['state'].unsqueeze(0)  # (1, 18, 8, 8)\n",
    "            mask = step['mask'].unsqueeze(0)    # (1, 4096)\n",
    "            action = step['action']\n",
    "            turn = step['turn']  # WHITE=True, BLACK=False\n",
    "            \n",
    "            # 현재 플레이어 관점의 결과\n",
    "            # 백 차례면 result 그대로, 흑 차례면 -result\n",
    "            player_result = result if turn else -result\n",
    "            \n",
    "            # Forward pass\n",
    "            policy_logits, value = model(state, mask)\n",
    "            policy_logits = policy_logits.squeeze(0)  # (4096,)\n",
    "            value = value.squeeze()  # scalar\n",
    "            \n",
    "            # 마스킹된 log 확률 계산 (공통 함수 사용)\n",
    "            mask_squeezed = mask.squeeze(0)  # (4096,)\n",
    "            log_probs = get_masked_log_probs(policy_logits, mask_squeezed)\n",
    "            \n",
    "            # Advantage = Return - Value (baseline)\n",
    "            advantage = player_result - value.detach()\n",
    "            \n",
    "            # Policy Loss: -log π(a|s) × A(s,a)\n",
    "            policy_loss = -log_probs[action] * advantage\n",
    "            \n",
    "            # Value Loss: MSE(V(s), R)\n",
    "            value_loss = (value - player_result) ** 2\n",
    "            \n",
    "            # Entropy: 합법 수에 대해서만 정규화된 엔트로피 계산 (공통 함수 사용)\n",
    "            entropy = compute_masked_entropy(policy_logits, mask_squeezed, device)\n",
    "            \n",
    "            total_policy_loss += policy_loss\n",
    "            total_value_loss += value_loss\n",
    "            total_entropy += entropy\n",
    "            total_steps += 1\n",
    "    \n",
    "    # 평균 계산\n",
    "    avg_policy_loss = total_policy_loss / total_steps\n",
    "    avg_value_loss = total_value_loss / total_steps\n",
    "    avg_entropy = total_entropy / total_steps\n",
    "    \n",
    "    # Total Loss = Policy Loss + c1 * Value Loss - c2 * Entropy\n",
    "    total_loss = avg_policy_loss + value_loss_weight * avg_value_loss - entropy_bonus * avg_entropy\n",
    "    \n",
    "    return total_loss, avg_policy_loss, avg_value_loss, avg_entropy\n",
    "\n",
    "\n",
    "def train_step(model, optimizer, trajectories, results, device,\n",
    "               value_loss_weight=0.5, entropy_bonus=0.01, max_grad_norm=1.0):\n",
    "    \"\"\"\n",
    "    한 번의 학습 스텝을 수행합니다.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss, policy_loss, value_loss, entropy = compute_a2c_loss(\n",
    "        model, trajectories, results, device, value_loss_weight, entropy_bonus\n",
    "    )\n",
    "    \n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Gradient Clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss.item(),\n",
    "        'policy_loss': policy_loss.item(),\n",
    "        'value_loss': value_loss.item(),\n",
    "        'entropy': entropy.item()\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"A2C 학습 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T20:07:09.449425100Z",
     "start_time": "2026-02-03T20:07:08.674331200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "A2C 강화학습 설정\n",
      "============================================================\n",
      "  Learning Rate: 0.0001\n",
      "  Iterations: 10000\n",
      "  Games per Iteration: 12\n",
      "  Num Envs (Batch Size): 12 (배치 GPU 추론)\n",
      "  Evaluation: 10 games every 10 iterations (승률 50%+)\n",
      "  Temperature: 1.0 → 0.5\n",
      "  Value Loss Weight: 0.5\n",
      "  Entropy Bonus: 0.01\n",
      "  Gradient Clipping: 1.0\n",
      "\n",
      "TensorBoard: tensorboard --logdir=models/tensorboard_rl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 학습 하이퍼파라미터\n",
    "# =============================================================================\n",
    "LEARNING_RATE = 1e-4           # Pre-trained 모델 fine-tuning용 낮은 학습률\n",
    "NUM_ITERATIONS = 10000          # 학습 반복 횟수\n",
    "GAMES_PER_ITERATION = 12       # 반복당 Self-play 게임 수\n",
    "NUM_ENVS = 12                   # 동시에 진행할 게임 수 (배치 크기)\n",
    "                                # num_envs=1: 순차 실행 (기존 동작, 롤백용)\n",
    "                                # num_envs>1: 배치 GPU 추론 (32 권장)\n",
    "EVAL_GAMES = 10                # 평가용 게임 수\n",
    "EVAL_INTERVAL = 10             # 평가 주기 (iteration)\n",
    "WIN_RATE_THRESHOLD = 0.5       # 승률 기준 (50% 이상이면 저장)\n",
    "TEMPERATURE_START = 1.0        # 시작 탐색 온도\n",
    "TEMPERATURE_END = 0.5          # 종료 탐색 온도\n",
    "VALUE_LOSS_WEIGHT = 0.5        # Value Loss 가중치\n",
    "ENTROPY_BONUS = 0.01           # 엔트로피 보너스\n",
    "MAX_GRAD_NORM = 1.0            # Gradient Clipping\n",
    "\n",
    "# 모델 저장 경로\n",
    "MODEL_DIR = Path(\"models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "RL_MODEL_PATH = MODEL_DIR / \"chess_cnn_rl_a2c.pth\"\n",
    "BEST_RL_MODEL_PATH = MODEL_DIR / \"best_chess_cnn_rl_a2c.pth\"\n",
    "\n",
    "# TensorBoard 로그\n",
    "LOG_DIR = MODEL_DIR / \"tensorboard_rl\"\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning Rate Scheduler (optional)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"A2C 강화학습 설정\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"  Games per Iteration: {GAMES_PER_ITERATION}\")\n",
    "print(f\"  Num Envs (Batch Size): {NUM_ENVS} {'(순차 실행)' if NUM_ENVS == 1 else '(배치 GPU 추론)'}\")\n",
    "print(f\"  Evaluation: {EVAL_GAMES} games every {EVAL_INTERVAL} iterations (승률 {WIN_RATE_THRESHOLD*100:.0f}%+)\")\n",
    "print(f\"  Temperature: {TEMPERATURE_START} → {TEMPERATURE_END}\")\n",
    "print(f\"  Value Loss Weight: {VALUE_LOSS_WEIGHT}\")\n",
    "print(f\"  Entropy Bonus: {ENTROPY_BONUS}\")\n",
    "print(f\"  Gradient Clipping: {MAX_GRAD_NORM}\")\n",
    "print(f\"\\nTensorBoard: tensorboard --logdir={LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-03T20:07:09.888217100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1/10000 (4.0s)\n",
      "  Loss: -1.1078 (P: -1.3930, V: 0.5730)\n",
      "  Entropy: 0.1258, Temperature: 1.00\n",
      "  Games: W3/B6/D3, Avg Moves: 106.8\n"
     ]
    }
   ],
   "source": [
    "# 학습 히스토리\n",
    "history = {\n",
    "    'total_loss': [],\n",
    "    'policy_loss': [],\n",
    "    'value_loss': [],\n",
    "    'entropy': [],\n",
    "    'avg_moves': [],\n",
    "    'white_wins': [],\n",
    "    'black_wins': [],\n",
    "    'draws': []\n",
    "}\n",
    "\n",
    "# 이전 최고 모델 저장용 (초기값은 현재 모델)\n",
    "best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"A2C 강화학습 시작\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for iteration in range(1, NUM_ITERATIONS + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Temperature 스케줄링 (선형 감소)\n",
    "    progress = (iteration - 1) / max(NUM_ITERATIONS - 1, 1)\n",
    "    temperature = TEMPERATURE_START + (TEMPERATURE_END - TEMPERATURE_START) * progress\n",
    "    \n",
    "    # 1. Self-play로 게임 생성\n",
    "    trajectories, results, stats = play_games_batch(\n",
    "        model, device, GAMES_PER_ITERATION, temperature, num_envs=NUM_ENVS\n",
    "    )\n",
    "    \n",
    "    # 2. A2C 학습\n",
    "    metrics = train_step(\n",
    "        model, optimizer, trajectories, results, device,\n",
    "        VALUE_LOSS_WEIGHT, ENTROPY_BONUS, MAX_GRAD_NORM\n",
    "    )\n",
    "    \n",
    "    # 3. Learning Rate 스케줄링\n",
    "    scheduler.step()\n",
    "    \n",
    "    # 4. 히스토리 저장\n",
    "    history['total_loss'].append(metrics['total_loss'])\n",
    "    history['policy_loss'].append(metrics['policy_loss'])\n",
    "    history['value_loss'].append(metrics['value_loss'])\n",
    "    history['entropy'].append(metrics['entropy'])\n",
    "    history['avg_moves'].append(stats['avg_moves'])\n",
    "    history['white_wins'].append(stats['white_wins'])\n",
    "    history['black_wins'].append(stats['black_wins'])\n",
    "    history['draws'].append(stats['draws'])\n",
    "    \n",
    "    # 5. TensorBoard 로깅\n",
    "    writer.add_scalar('Loss/Total', metrics['total_loss'], iteration)\n",
    "    writer.add_scalar('Loss/Policy', metrics['policy_loss'], iteration)\n",
    "    writer.add_scalar('Loss/Value', metrics['value_loss'], iteration)\n",
    "    writer.add_scalar('Entropy', metrics['entropy'], iteration)\n",
    "    writer.add_scalar('Games/Avg_Moves', stats['avg_moves'], iteration)\n",
    "    writer.add_scalar('Games/White_Wins', stats['white_wins'], iteration)\n",
    "    writer.add_scalar('Games/Black_Wins', stats['black_wins'], iteration)\n",
    "    writer.add_scalar('Games/Draws', stats['draws'], iteration)\n",
    "    writer.add_scalar('Temperature', temperature, iteration)\n",
    "    writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], iteration)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # 6. 진행 상황 출력\n",
    "    if iteration % 5 == 0 or iteration == 1:\n",
    "        print(f\"\\nIteration {iteration}/{NUM_ITERATIONS} ({elapsed:.1f}s)\")\n",
    "        print(f\"  Loss: {metrics['total_loss']:.4f} (P: {metrics['policy_loss']:.4f}, V: {metrics['value_loss']:.4f})\")\n",
    "        print(f\"  Entropy: {metrics['entropy']:.4f}, Temperature: {temperature:.2f}\")\n",
    "        print(f\"  Games: W{stats['white_wins']}/B{stats['black_wins']}/D{stats['draws']}, Avg Moves: {stats['avg_moves']:.1f}\")\n",
    "    \n",
    "    # 7. 승률 기반 최고 모델 저장\n",
    "    if iteration % EVAL_INTERVAL == 0:\n",
    "        # 이전 최고 모델 로드\n",
    "        opponent = ChessCNN(num_channels=256).to(device)\n",
    "        opponent.load_state_dict(best_model_state)\n",
    "        opponent.eval()\n",
    "        \n",
    "        # 대결\n",
    "        win_rate = evaluate_vs_opponent(model, opponent, device, EVAL_GAMES, temperature=0.5)\n",
    "        \n",
    "        if win_rate >= WIN_RATE_THRESHOLD:\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': metrics['total_loss'],\n",
    "                'win_rate': win_rate,\n",
    "                'temperature': temperature,\n",
    "            }, BEST_RL_MODEL_PATH)\n",
    "            if iteration % 5 == 0 or iteration == 1:\n",
    "                print(f\"  ✅ 새 최고 모델! 승률: {win_rate*100:.1f}%\")\n",
    "        else:\n",
    "            if iteration % 5 == 0 or iteration == 1:\n",
    "                print(f\"  ⚠️  승률 부족: {win_rate*100:.1f}% < {WIN_RATE_THRESHOLD*100:.0f}%\")\n",
    "    \n",
    "    # 8. 주기적 체크포인트 저장\n",
    "    if iteration % 10 == 0:\n",
    "        torch.save({\n",
    "            'iteration': iteration,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': metrics['total_loss'],\n",
    "            'history': history,\n",
    "        }, RL_MODEL_PATH)\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save({\n",
    "    'iteration': NUM_ITERATIONS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'loss': history['total_loss'][-1],\n",
    "    'history': history,\n",
    "}, RL_MODEL_PATH)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"학습 완료!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"최종 Loss: {history['total_loss'][-1]:.4f}\")\n",
    "print(f\"모델 저장: {RL_MODEL_PATH}\")\n",
    "print(f\"최고 모델: {BEST_RL_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 곡선 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Total Loss\n",
    "axes[0, 0].plot(history['total_loss'], label='Total Loss', color='blue')\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# 2. Policy & Value Loss\n",
    "axes[0, 1].plot(history['policy_loss'], label='Policy Loss', color='green')\n",
    "axes[0, 1].plot(history['value_loss'], label='Value Loss', color='red')\n",
    "axes[0, 1].set_title('Policy & Value Loss')\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# 3. Entropy\n",
    "axes[0, 2].plot(history['entropy'], label='Entropy', color='purple')\n",
    "axes[0, 2].set_title('Policy Entropy')\n",
    "axes[0, 2].set_xlabel('Iteration')\n",
    "axes[0, 2].set_ylabel('Entropy')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True)\n",
    "\n",
    "# 4. Average Moves per Game\n",
    "axes[1, 0].plot(history['avg_moves'], label='Avg Moves', color='orange')\n",
    "axes[1, 0].set_title('Average Moves per Game')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Moves')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# 5. Win/Loss/Draw\n",
    "iterations = range(1, len(history['white_wins']) + 1)\n",
    "axes[1, 1].plot(iterations, history['white_wins'], label='White Wins', color='white', \n",
    "                marker='o', markerfacecolor='gray', markeredgecolor='black')\n",
    "axes[1, 1].plot(iterations, history['black_wins'], label='Black Wins', color='black', marker='s')\n",
    "axes[1, 1].plot(iterations, history['draws'], label='Draws', color='gray', marker='^')\n",
    "axes[1, 1].set_title('Game Results')\n",
    "axes[1, 1].set_xlabel('Iteration')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "axes[1, 1].set_facecolor('#f0f0f0')\n",
    "\n",
    "# 6. Win Rate (Stacked)\n",
    "total_games = [w + b + d for w, b, d in zip(history['white_wins'], history['black_wins'], history['draws'])]\n",
    "white_rate = [w / t * 100 for w, t in zip(history['white_wins'], total_games)]\n",
    "black_rate = [b / t * 100 for b, t in zip(history['black_wins'], total_games)]\n",
    "draw_rate = [d / t * 100 for d, t in zip(history['draws'], total_games)]\n",
    "\n",
    "axes[1, 2].stackplot(iterations, white_rate, black_rate, draw_rate, \n",
    "                     labels=['White', 'Black', 'Draw'],\n",
    "                     colors=['#f0f0f0', '#404040', '#808080'])\n",
    "axes[1, 2].set_title('Win Rate (%)')\n",
    "axes[1, 2].set_xlabel('Iteration')\n",
    "axes[1, 2].set_ylabel('Percentage')\n",
    "axes[1, 2].legend(loc='upper right')\n",
    "axes[1, 2].set_ylim(0, 100)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'rl_training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"학습 곡선 저장: {MODEL_DIR / 'rl_training_curves.png'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
