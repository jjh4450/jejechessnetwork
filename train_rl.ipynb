{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ® ì²´ìŠ¤ AI ê°•í™”í•™ìŠµ (A2C)\n",
    "\n",
    "**Pre-trained CNN ëª¨ë¸ì„ A2C ì•Œê³ ë¦¬ì¦˜ê³¼ Self-playë¡œ ê°•í™”í•™ìŠµí•©ë‹ˆë‹¤.**\n",
    "\n",
    "## í•™ìŠµ íŒŒì´í”„ë¼ì¸\n",
    "1. ì§€ë„í•™ìŠµ (train_cnn.ipynb) â†’ Pre-trained ëª¨ë¸\n",
    "2. **ê°•í™”í•™ìŠµ (ì´ ë…¸íŠ¸ë¶)** â†’ RL ëª¨ë¸\n",
    "3. í‰ê°€ (eval_vs_stockfish.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:01:50.430945800Z",
     "start_time": "2026-02-05T10:01:48.647910300Z"
    }
   },
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ\n",
    "from chess_model import ChessCNN, load_model\n",
    "from train_utils import (\n",
    "    compute_a2c_loss, train_step, play_games_batch, \n",
    "    evaluate_vs_opponent, check_weight_diff,\n",
    "    compute_kl_divergence, compute_masked_entropy\n",
    ")\n",
    "import fast_chess as fc\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# CUDA ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: cuda\n",
      "ğŸ“Š GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "ğŸ’¾ VRAM: 17.1 GB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:01:50.487037Z",
     "start_time": "2026-02-05T10:01:50.436349Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "# =============================================================================\n",
    "LEARNING_RATE = 1e-4          # Fine-tuningìš© ë‚®ì€ í•™ìŠµë¥ \n",
    "NUM_ITERATIONS = 10000        # ì´ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜\n",
    "GAMES_PER_ITERATION = 12      # ë°˜ë³µë‹¹ Self-play ê²Œì„ ìˆ˜\n",
    "NUM_ENVS = 12                 # ë™ì‹œ ì§„í–‰ ê²Œì„ ìˆ˜ (ë°°ì¹˜ í¬ê¸°)\n",
    "MAX_MOVES = 200               # ê²Œì„ë‹¹ ìµœëŒ€ ìˆ˜\n",
    "\n",
    "# =============================================================================\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜\n",
    "# =============================================================================\n",
    "VALUE_LOSS_WEIGHT = 0.5       # Value Loss ê°€ì¤‘ì¹˜\n",
    "ENTROPY_BONUS = 0.01          # ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ (íƒìƒ‰ ì¥ë ¤)\n",
    "KL_PENALTY = 0.1              # KL Divergence íŒ¨ë„í‹° ì´ˆê¸°ê°’\n",
    "\n",
    "# =============================================================================\n",
    "# KL Divergence ì„¤ì • (Adaptive)\n",
    "# =============================================================================\n",
    "USE_ADAPTIVE_KL = True        # Adaptive KL íŒ¨ë„í‹° ì‚¬ìš©\n",
    "KL_TARGET = 0.001             # ëª©í‘œ KL ê°’\n",
    "KL_PENALTY_MIN = 1e-4         # ìµœì†Œ Î² ê°’\n",
    "KL_PENALTY_MAX = 10.0         # ìµœëŒ€ Î² ê°’\n",
    "\n",
    "# =============================================================================\n",
    "# Temperature ìŠ¤ì¼€ì¤„\n",
    "# =============================================================================\n",
    "TEMPERATURE_START = 1.0       # ì‹œì‘ íƒìƒ‰ ì˜¨ë„\n",
    "TEMPERATURE_END = 0.5         # ì¢…ë£Œ íƒìƒ‰ ì˜¨ë„\n",
    "\n",
    "# =============================================================================\n",
    "# í‰ê°€ ë° ì €ì¥\n",
    "# =============================================================================\n",
    "EVAL_INTERVAL = 10            # í‰ê°€ ì£¼ê¸° (iteration)\n",
    "EVAL_GAMES = 10               # í‰ê°€ìš© ëŒ€ê²° ê²Œì„ ìˆ˜\n",
    "WIN_RATE_THRESHOLD = 0.5      # ì €ì¥ ê¸°ì¤€ ìŠ¹ë¥  (50%+)\n",
    "SAVE_INTERVAL = 50            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°\n",
    "\n",
    "# =============================================================================\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "# =============================================================================\n",
    "PRETRAINED_PATH = 'models/best_chess_cnn.pth'\n",
    "CHECKPOINT_PATH = 'models/chess_cnn_rl_a2c.pth'\n",
    "BEST_MODEL_PATH = 'models/best_chess_cnn_rl_a2c.pth'\n",
    "TENSORBOARD_DIR = 'models/tensorboard_rl'\n",
    "\n",
    "print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"   - í•™ìŠµë¥ : {LEARNING_RATE}\")\n",
    "print(f\"   - ë°˜ë³µ íšŸìˆ˜: {NUM_ITERATIONS}\")\n",
    "print(f\"   - ê²Œì„/ë°˜ë³µ: {GAMES_PER_ITERATION}\")\n",
    "print(f\"   - ë™ì‹œ ê²Œì„: {NUM_ENVS}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\n",
      "   - í•™ìŠµë¥ : 0.0001\n",
      "   - ë°˜ë³µ íšŸìˆ˜: 10000\n",
      "   - ê²Œì„/ë°˜ë³µ: 12\n",
      "   - ë™ì‹œ ê²Œì„: 12\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ë¡œë“œ\n",
    "\n",
    "Pre-trained ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , Reference ëª¨ë¸(ê³ ì •)ê³¼ í•™ìŠµ ëª¨ë¸ì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:01:51.105140100Z",
     "start_time": "2026-02-05T10:01:50.488092200Z"
    }
   },
   "source": [
    "# Pre-trained ëª¨ë¸ ë¡œë“œ\n",
    "print(\"ğŸ“¥ Pre-trained ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "if os.path.exists(PRETRAINED_PATH):\n",
    "    model, checkpoint = load_model(PRETRAINED_PATH, device)\n",
    "    print(f\"   âœ… {PRETRAINED_PATH} ë¡œë“œ ì™„ë£Œ\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ {PRETRAINED_PATH} ì—†ìŒ - ìƒˆ ëª¨ë¸ ìƒì„±\")\n",
    "    model = ChessCNN(num_channels=256).to(device)\n",
    "\n",
    "# í•™ìŠµ ëª¨ë¸ë¡œ ì„¤ì •\n",
    "model.train()\n",
    "\n",
    "# Reference ëª¨ë¸ (Pre-trained ìƒíƒœ ê³ ì •)\n",
    "print(\"ğŸ”’ Reference ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "ref_model = ChessCNN(num_channels=256).to(device)\n",
    "ref_model.load_state_dict(model.state_dict())\n",
    "ref_model.eval()\n",
    "\n",
    "# Reference ëª¨ë¸ íŒŒë¼ë¯¸í„° ë™ê²°\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"   âœ… Reference ëª¨ë¸ ë™ê²° ì™„ë£Œ\")\n",
    "\n",
    "# Best ëª¨ë¸ (í‰ê°€ìš© ìƒëŒ€)\n",
    "best_model = ChessCNN(num_channels=256).to(device)\n",
    "best_model.load_state_dict(model.state_dict())\n",
    "best_model.eval()\n",
    "\n",
    "print(\"   âœ… Best ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°:\")\n",
    "print(f\"   - ì „ì²´: {total_params:,}\")\n",
    "print(f\"   - í•™ìŠµ ê°€ëŠ¥: {trainable_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… models/best_chess_cnn.pth ë¡œë“œ ì™„ë£Œ\n",
      "ğŸ”’ Reference ëª¨ë¸ ìƒì„± ì¤‘...\n",
      "   âœ… Reference ëª¨ë¸ ë™ê²° ì™„ë£Œ\n",
      "   âœ… Best ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "\n",
      "ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°:\n",
      "   - ì „ì²´: 9,314,433\n",
      "   - í•™ìŠµ ê°€ëŠ¥: 9,314,433\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì˜µí‹°ë§ˆì´ì € ë° TensorBoard ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:01:52.585057400Z",
     "start_time": "2026-02-05T10:01:51.817118400Z"
    }
   },
   "source": [
    "# ì˜µí‹°ë§ˆì´ì €\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# AMP (Automatic Mixed Precision)\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "print(f\"âš¡ AMP: {'í™œì„±í™”' if use_amp else 'ë¹„í™œì„±í™”'}\")\n",
    "\n",
    "# TensorBoard\n",
    "os.makedirs(TENSORBOARD_DIR, exist_ok=True)\n",
    "writer = SummaryWriter(TENSORBOARD_DIR)\n",
    "print(f\"ğŸ“Š TensorBoard: {TENSORBOARD_DIR}\")\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ\n",
    "start_iteration = 0\n",
    "best_win_rate = 0.0\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=False)\n",
    "    if 'model_state_dict' in ckpt:\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        if 'optimizer_state_dict' in ckpt:\n",
    "            optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        start_iteration = ckpt.get('iteration', 0)\n",
    "        best_win_rate = ckpt.get('best_win_rate', 0.0)\n",
    "        KL_PENALTY = ckpt.get('kl_penalty', KL_PENALTY)\n",
    "        print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: iteration {start_iteration}, win_rate {best_win_rate:.2%}\")\n",
    "\n",
    "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ (iteration {start_iteration}ë¶€í„°)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ AMP: í™œì„±í™”\n",
      "ğŸ“Š TensorBoard: models/tensorboard_rl\n",
      "\n",
      "ğŸš€ í•™ìŠµ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ (iteration 0ë¶€í„°)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•™ìŠµ ë£¨í”„\n",
    "\n",
    "Self-playë¡œ ê²Œì„ì„ ì§„í–‰í•˜ê³  A2C ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-05T10:01:52.978535200Z"
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"ğŸ® ê°•í™”í•™ìŠµ ì‹œì‘!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í˜„ì¬ KL Penalty (Adaptiveìš©)\n",
    "current_kl_penalty = KL_PENALTY\n",
    "\n",
    "for iteration in tqdm(range(start_iteration, NUM_ITERATIONS), desc=\"í•™ìŠµ ì§„í–‰\"):\n",
    "    # =========================================================================\n",
    "    # Temperature ìŠ¤ì¼€ì¤„ (ì„ í˜• ê°ì†Œ)\n",
    "    # =========================================================================\n",
    "    progress = iteration / max(NUM_ITERATIONS - 1, 1)\n",
    "    temperature = TEMPERATURE_START + (TEMPERATURE_END - TEMPERATURE_START) * progress\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Self-play ê²Œì„ ì§„í–‰\n",
    "    # =========================================================================\n",
    "    trajectories, results, game_stats = play_games_batch(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        num_games=GAMES_PER_ITERATION,\n",
    "        temperature=temperature,\n",
    "        num_envs=NUM_ENVS,\n",
    "        max_moves=MAX_MOVES\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # A2C í•™ìŠµ ìŠ¤í…\n",
    "    # =========================================================================\n",
    "    train_metrics = train_step(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        optimizer=optimizer,\n",
    "        trajectories=trajectories,\n",
    "        results=results,\n",
    "        device=device,\n",
    "        value_loss_weight=VALUE_LOSS_WEIGHT,\n",
    "        entropy_bonus=ENTROPY_BONUS,\n",
    "        kl_penalty=current_kl_penalty,\n",
    "        max_grad_norm=1.0,\n",
    "        scaler=scaler,\n",
    "        use_amp=use_amp\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Adaptive KL Penalty\n",
    "    # =========================================================================\n",
    "    if USE_ADAPTIVE_KL:\n",
    "        avg_kl = train_metrics['kl_div']\n",
    "        if avg_kl > KL_TARGET * 1.5:\n",
    "            current_kl_penalty = min(current_kl_penalty * 1.5, KL_PENALTY_MAX)\n",
    "        elif avg_kl < KL_TARGET * 0.5:\n",
    "            current_kl_penalty = max(current_kl_penalty / 1.5, KL_PENALTY_MIN)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TensorBoard ë¡œê¹…\n",
    "    # =========================================================================\n",
    "    writer.add_scalar('Loss/Total', train_metrics['total_loss'], iteration)\n",
    "    writer.add_scalar('Loss/Policy', train_metrics['policy_loss'], iteration)\n",
    "    writer.add_scalar('Loss/Value', train_metrics['value_loss'], iteration)\n",
    "    writer.add_scalar('Entropy', train_metrics['entropy'], iteration)\n",
    "    writer.add_scalar('KL_Divergence', train_metrics['kl_div'], iteration)\n",
    "    writer.add_scalar('KL_Penalty', current_kl_penalty, iteration)\n",
    "    writer.add_scalar('Temperature', temperature, iteration)\n",
    "    writer.add_scalar('Games/Avg_Moves', game_stats['avg_moves'], iteration)\n",
    "    writer.add_scalar('Games/White_Wins', game_stats['white_wins'], iteration)\n",
    "    writer.add_scalar('Games/Black_Wins', game_stats['black_wins'], iteration)\n",
    "    writer.add_scalar('Games/Draws', game_stats['draws'], iteration)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # í‰ê°€ (ì£¼ê¸°ì )\n",
    "    # =========================================================================\n",
    "    if (iteration + 1) % EVAL_INTERVAL == 0:\n",
    "        win_rate = evaluate_vs_opponent(\n",
    "            current_model=model,\n",
    "            opponent_model=best_model,\n",
    "            device=device,\n",
    "            num_games=EVAL_GAMES,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        writer.add_scalar('Win_Rate', win_rate, iteration)\n",
    "        \n",
    "        # ìµœê³  ëª¨ë¸ ê°±ì‹ \n",
    "        if win_rate >= WIN_RATE_THRESHOLD and win_rate > best_win_rate:\n",
    "            best_win_rate = win_rate\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "            # Best ëª¨ë¸ ì €ì¥\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'iteration': iteration,\n",
    "                'win_rate': win_rate,\n",
    "                'kl_penalty': current_kl_penalty,\n",
    "            }, BEST_MODEL_PATH)\n",
    "            \n",
    "            tqdm.write(f\"ğŸ† [Iter {iteration+1}] ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸! Win Rate: {win_rate:.2%}\")\n",
    "        else:\n",
    "            tqdm.write(f\"ğŸ“Š [Iter {iteration+1}] Win Rate: {win_rate:.2%} (Best: {best_win_rate:.2%})\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì£¼ê¸°ì )\n",
    "    # =========================================================================\n",
    "    if (iteration + 1) % SAVE_INTERVAL == 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'iteration': iteration + 1,\n",
    "            'best_win_rate': best_win_rate,\n",
    "            'kl_penalty': current_kl_penalty,\n",
    "        }, CHECKPOINT_PATH)\n",
    "        tqdm.write(f\"ğŸ’¾ [Iter {iteration+1}] ì²´í¬í¬ì¸íŠ¸ ì €ì¥\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ì§„í–‰ ìƒí™© ì¶œë ¥ (100 iterationë§ˆë‹¤)\n",
    "    # =========================================================================\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        weight_diff = check_weight_diff(model, ref_model)\n",
    "        tqdm.write(f\"ğŸ“ˆ [Iter {iteration+1}] Loss: {train_metrics['total_loss']:.4f}, \"\n",
    "                   f\"Entropy: {train_metrics['entropy']:.4f}, \"\n",
    "                   f\"KL: {train_metrics['kl_div']:.6f}, \"\n",
    "                   f\"Moves: {game_stats['avg_moves']:.1f}, \"\n",
    "                   f\"Weight Diff: {weight_diff:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# ìµœì¢… ì €ì¥\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'iteration': NUM_ITERATIONS,\n",
    "    'best_win_rate': best_win_rate,\n",
    "    'kl_penalty': current_kl_penalty,\n",
    "}, CHECKPOINT_PATH)\n",
    "print(f\"ğŸ’¾ ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {CHECKPOINT_PATH}\")\n",
    "\n",
    "writer.close()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ì§„í–‰:   0%|          | 23/10000 [03:00<21:05:13,  7.61s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ê²°ê³¼ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ê°€ì¤‘ì¹˜ ë³€í™” í™•ì¸\n",
    "weight_diff = check_weight_diff(model, ref_model)\n",
    "print(f\"ğŸ“Š í•™ìŠµ ê²°ê³¼ ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   - Reference ëª¨ë¸ê³¼ì˜ ê°€ì¤‘ì¹˜ ì°¨ì´: {weight_diff:.6f}\")\n",
    "\n",
    "if weight_diff < 1e-6:\n",
    "    print(\"   âš ï¸ ê²½ê³ : ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ê±°ì˜ ë³€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n",
    "    print(\"      â†’ í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ê±°ë‚˜, KL íŒ¨ë„í‹°ê°€ ë„ˆë¬´ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"   âœ… ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(f\"\\n   - ìµœê³  ìŠ¹ë¥ : {best_win_rate:.2%}\")\n",
    "print(f\"   - ìµœì¢… KL íŒ¨ë„í‹°: {current_kl_penalty:.6f}\")\n",
    "\n",
    "# ìµœì¢… í‰ê°€\n",
    "print(\"\\nğŸ¯ ìµœì¢… í‰ê°€ (vs Best Model)...\")\n",
    "final_win_rate = evaluate_vs_opponent(\n",
    "    current_model=model,\n",
    "    opponent_model=best_model,\n",
    "    device=device,\n",
    "    num_games=20,\n",
    "    temperature=0.3\n",
    ")\n",
    "print(f\"   - ìŠ¹ë¥ : {final_win_rate:.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ìƒ˜í”Œ ê²Œì„ ì‹œê°í™”\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒ˜í”Œ ê²Œì„ì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def play_demo_game(model, device, temperature=0.3, max_moves=100, verbose=True):\n",
    "    \"\"\"ìƒ˜í”Œ ê²Œì„ì„ ì§„í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    import fast_chess as fc\n",
    "    \n",
    "    state = fc.create_initial_state()\n",
    "    model.eval()\n",
    "    \n",
    "    moves_history = []\n",
    "    \n",
    "    for move_num in range(max_moves):\n",
    "        if fc.is_game_over(state):\n",
    "            break\n",
    "        \n",
    "        state_tensor = torch.from_numpy(fc.board_to_tensor_fast(state))\n",
    "        mask_tensor = torch.from_numpy(fc.legal_move_mask_fast(state))\n",
    "        \n",
    "        if mask_tensor.sum() == 0:\n",
    "            break\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_input = state_tensor.unsqueeze(0).to(device)\n",
    "            mask_input = mask_tensor.unsqueeze(0).to(device)\n",
    "            policy_logits, value_pred = model(state_input, mask_input)\n",
    "        \n",
    "        # Greedy ì„ íƒ (temperature=0.3)\n",
    "        from train_utils import select_action\n",
    "        action, _, _ = select_action(policy_logits.squeeze(0), mask_tensor.to(device), temperature)\n",
    "        \n",
    "        # ìˆ˜ ë³€í™˜\n",
    "        from_sq = action // 64\n",
    "        to_sq = action % 64\n",
    "        files = \"abcdefgh\"\n",
    "        ranks = \"12345678\"\n",
    "        move_str = f\"{files[from_sq & 7]}{ranks[from_sq >> 3]}{files[to_sq & 7]}{ranks[to_sq >> 3]}\"\n",
    "        moves_history.append(move_str)\n",
    "        \n",
    "        fc.make_move(state, action)\n",
    "    \n",
    "    # ê²°ê³¼\n",
    "    result = fc.get_result(state)\n",
    "    if result == 1.0:\n",
    "        result_str = \"1-0 (ë°± ìŠ¹)\"\n",
    "    elif result == 0.0:\n",
    "        result_str = \"0-1 (í‘ ìŠ¹)\"\n",
    "    else:\n",
    "        result_str = \"1/2-1/2 (ë¬´ìŠ¹ë¶€)\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ğŸ® ìƒ˜í”Œ ê²Œì„ ê²°ê³¼: {result_str}\")\n",
    "        print(f\"   ì´ {len(moves_history)}ìˆ˜\")\n",
    "        print(f\"   ìˆ˜ìˆœ: {' '.join(moves_history[:20])}{'...' if len(moves_history) > 20 else ''}\")\n",
    "    \n",
    "    return moves_history, result_str\n",
    "\n",
    "# ìƒ˜í”Œ ê²Œì„ 3ê°œ ì§„í–‰\n",
    "print(\"ğŸ² ìƒ˜í”Œ ê²Œì„ ì§„í–‰ ì¤‘...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nê²Œì„ #{i+1}\")\n",
    "    play_demo_game(model, device, temperature=0.3)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
